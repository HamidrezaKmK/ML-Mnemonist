{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HamidrezaKmK/ML-Mnemonist/blob/main/ExperimentRunnerTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE6olttuFV0Q"
   },
   "source": [
    "# Setup hosted runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOfp_NGbT1jZ",
    "outputId": "79649df5-19a7-4f4a-a2a6-f49a6cc498c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sys_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sys_setup.py\n",
    "#!usr/bin/bash python\n",
    "\n",
    "import sys\n",
    "from google.colab import drive\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "PROJ_NAME = 'ML-Mnemonist'\n",
    "GIT_DIR = 'HamidrezaKmK'\n",
    "\n",
    "data_dir = None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  env = os.environ.copy()\n",
    "  if not os.path.exists('/content/drive'):\n",
    "    print(\"Mounting drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Mount complete!\")\n",
    "\n",
    "  while True:\n",
    "    opt = input(\"What are you trying to do? [clone/pull] \")\n",
    "    if opt == 'clone':\n",
    "      addr = f\"https://github.com/{GIT_DIR}/{PROJ_NAME}\"\n",
    "      print(f\"Trying to connect to {addr}\")\n",
    "      token = input(\"Enter token: \")\n",
    "      addr = addr.replace('[TOKEN]', token)\n",
    "      res = subprocess.run(['git', 'clone', addr], env=env, capture_output=True)\n",
    "      print(res.stdout.decode())\n",
    "      print(res.stderr.decode())\n",
    "      break\n",
    "    elif opt == 'pull':\n",
    "      path = os.path.join('/content', PROJ_NAME)\n",
    "      os.chdir(path)\n",
    "      res = subprocess.run(['git', 'pull'], env=env, capture_output=True)\n",
    "      print(res.stdout.decode())\n",
    "      print(res.stderr.decode())\n",
    "      break\n",
    "    elif opt == '':\n",
    "      print(\"Nothing happened!\")\n",
    "      break\n",
    "  \n",
    "  if not os.path.exists(f'/content/{PROJ_NAME}'):\n",
    "    raise RuntimeError(\"No project repository available!\")\n",
    "\n",
    "  if not os.path.exists(f'/content/{PROJ_NAME}/.env'):\n",
    "    print(\"Dotenv non-existant!\")\n",
    "    while True:\n",
    "      resp = input(\"Do you want to enter the file in the prompt or copy it?\\n[copy/write] \")\n",
    "      if resp == 'copy':\n",
    "        dir = input(\"Enter the directory to copy: \")\n",
    "        shutil.copyfile(dir, f'/content/{PROJ_NAME}/.env')\n",
    "      elif resp == 'write':\n",
    "        print(\"Enter the lines in format ENV_VARIABLE_NAME=VALUE\")\n",
    "        print(\"End with: ENDFILE\")\n",
    "        with open(f'/content/{PROJ_NAME}/.env', 'w') as f:\n",
    "          while True:\n",
    "            line = input()\n",
    "            if line == 'ENDFILE':\n",
    "              break\n",
    "            f.write(f'{line}\\n')\n",
    "      else:\n",
    "        continue\n",
    "      break\n",
    "        \n",
    "  os.chdir('/content')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "mzkBQqSLUWC3",
    "outputId": "2d475291-ff6f-430d-da51-06a1eb22bb66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROJ_NAME = 'ML-Mnemonist'\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%run /content/sys_setup.py\n",
    "resp = input(\"Do you want to install packages? [y/n] \")\n",
    "if resp == 'y':\n",
    "  !pip install -r /content/ML-Mnemonist/requirements.txt\n",
    "  input(\"Requrements installed! -- press any key to continue ...\")\n",
    "clear_output()\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'/content/{PROJ_NAME}')\n",
    "os.chdir(f'/content/{PROJ_NAME}')\n",
    "print(\"Running complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNiIuYQbWti3"
   },
   "source": [
    "# Expand configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FxBXOOvtLWC"
   },
   "source": [
    "The configuration available at `conf-test-branches.yaml`\n",
    "contains multiple scenarios for the configurations to occur.\n",
    "We may expand all the possible configurations using the function below\n",
    "to create all the possible outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "azqIcguUWxka"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mlmnemonist.validation_tools import expand_cfg\n",
    "from testing.config.config import get_cfg_defaults\n",
    "expand_cfg(get_cfg_defaults(),\n",
    "           cfg_dir='conf-test-branches.yaml',\n",
    "           save_directory='config/all-branches')\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's create the same model we created before\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GFRWSI6F_Af_"
   },
   "outputs": [],
   "source": [
    "from mlmnemonist.experiment_runner import ExperimentRunner\n",
    "from mlmnemonist.experiment_factory import ExperimentRunnerFactory\n",
    "import pandas as pd\n",
    "\n",
    "from testing.config.config import get_cfg_defaults\n",
    "import os\n",
    "\n",
    "factory = ExperimentRunnerFactory(cfg_base=get_cfg_defaults())\n",
    "\n",
    "\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "runner = factory.create(experiment_name='my-cv-fold',\n",
    "                        cfg_dir='conf-test.yaml',\n",
    "                        verbose=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "runner = factory.retrieve('0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runner at /home/hamidreza/myprojects/ML-Mnemonist/testing/mnemonic-experiments/2022-07-01-my-cv-fold-2\n",
      "\t - cache token: 0\n",
      "\t - configurations at: /home/hamidreza/myprojects/ML-Mnemonist/testing/config/conf-test.yaml\n",
      "\t - preprocessings functions []\n",
      "\t - recurring pipeline []\n",
      "\t - Run function not implemented!\n"
     ]
    }
   ],
   "source": [
    "print(runner)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fQaHIvGQ_3YG"
   },
   "outputs": [],
   "source": [
    "runner.preprocessing_pipeline.clear_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PhuvzGFG-86Z"
   },
   "outputs": [],
   "source": [
    "def load_raw_data(runner: ExperimentRunner):\n",
    "  train_name = runner.cfg.DATASET.TRAIN_NAME\n",
    "  test_name = runner.cfg.DATASET.TEST_NAME\n",
    "  runner.train_df = pd.read_csv(runner.reveal_true_path(train_name))\n",
    "  runner.test_df = pd.read_csv(runner.reveal_true_path(test_name))\n",
    "runner.preprocessing_pipeline.update_function(load_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3Vo6EsKg_jmk"
   },
   "outputs": [],
   "source": [
    "def process_data(runner: ExperimentRunner):\n",
    "  runner.train_X = runner.train_df.drop(columns='median_house_value').to_numpy()\n",
    "  runner.train_Y = runner.train_df['median_house_value'].to_numpy()\n",
    "  runner.test_X = runner.test_df.drop(columns='median_house_value').to_numpy()\n",
    "  runner.test_Y = runner.test_df['median_house_value'].to_numpy()\n",
    "runner.preprocessing_pipeline.update_function(process_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runner at /home/hamidreza/myprojects/ML-Mnemonist/testing/mnemonic-experiments/2022-07-01-my-cv-fold-2\n",
      "\t - cache token: 0\n",
      "\t - configurations at: /home/hamidreza/myprojects/ML-Mnemonist/testing/config/conf-test.yaml\n",
      "\t - preprocessings functions ['load_raw_data', 'process_data']\n",
      "\t - recurring pipeline []\n",
      "\t - Run function not implemented!\n"
     ]
    }
   ],
   "source": [
    "print(runner)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9XySh8pBGwz",
    "outputId": "279a1b1d-764f-4e9b-a8c5-a53d936e709e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Running load_raw_data\n",
      "[2/2] Running process_data\n"
     ]
    }
   ],
   "source": [
    "# Run the functions in line\n",
    "runner.preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4rUEXYL2p-Jx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    def __init__(self, input_features: int, hidden_layer_1: int, hidden_layer_2: int):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.l1 = nn.Linear(input_features, hidden_layer_1)\n",
    "        self.l2 = nn.Linear(hidden_layer_1, hidden_layer_2)\n",
    "        self.l3 = nn.Linear(hidden_layer_2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2DdIqwaOSRlR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def setup_device(runner: ExperimentRunner):\n",
    "  # extract the device from config\n",
    "  device = runner.cfg.SOLVER.DEVICE\n",
    "  if device == 'cpu':\n",
    "    runner.device = torch.device('cpu')\n",
    "  elif device == 'cuda' and torch.cuda.is_available():\n",
    "    runner.device = torch.device('cuda')\n",
    "  else:\n",
    "    raise NotImplementedError(f\"device {device} is not implemented!\")\n",
    "runner.recurring_pipeline.update_function(setup_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ort8OLJRIeUQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def setup_model(runner: ExperimentRunner):\n",
    "  # extract the method from config\n",
    "  method = runner.cfg.SOLVER.METHOD\n",
    "  # construct the model from registry\n",
    "  cfg_h_params = runner.cfg.MODEL.HYPER_PARAMETERS\n",
    "  my_model = MyMLP(input_features=cfg_h_params.IN_FEATURES,\n",
    "                   hidden_layer_1=cfg_h_params.H1,\n",
    "                   hidden_layer_2=cfg_h_params.H2)\n",
    "  # set the function in the cache to save weights\n",
    "  my_model = runner.CACHE.SET_M('mlp-key', my_model)\n",
    "  my_model.to(runner.device)\n",
    "  if runner.verbose > 0:\n",
    "    print(\"Model state dict\")\n",
    "    print(my_model)\n",
    "runner.recurring_pipeline.update_function(setup_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sGhQ3LKcS-dC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def setup_training(runner: ExperimentRunner):\n",
    "  my_model = runner.CACHE.GET_M('mlp-key').to(runner.device)\n",
    "  runner.criterion = nn.MSELoss()\n",
    "  if runner.cfg.SOLVER.OPTIMIZER_TYPE == 'adam':\n",
    "    runner.optim = torch.optim.Adam(my_model.parameters(), lr=runner.cfg.SOLVER.LR)\n",
    "  else:\n",
    "    raise NotImplementedError(f\"Optimizer type not implemented\"\n",
    "                              f\"{runner.cfg.SOLVER.OPTIMIZER_TYPE}\")\n",
    "runner.recurring_pipeline.update_function(setup_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3H9fjChzRRF",
    "outputId": "17f0f93e-66cd-4785-bfa7-99db66b52d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runner at /home/hamidreza/myprojects/ML-Mnemonist/testing/mnemonic-experiments/2022-07-01-my-cv-fold-2\n",
      "\t - cache token: 0\n",
      "\t - configurations at: /home/hamidreza/myprojects/ML-Mnemonist/testing/config/conf-test.yaml\n",
      "\t - preprocessings functions ['load_raw_data', 'process_data']\n",
      "\t - recurring pipeline ['setup_device', 'setup_model', 'setup_training']\n",
      "\t - Run function not implemented!\n"
     ]
    }
   ],
   "source": [
    "print(runner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57mO42YpH-Ck"
   },
   "source": [
    "# Implement a run function\n",
    "\n",
    "A run function is the core part of an experiment. This function takes in an input of type `ExperimentRunner` type is its first element and a bunch of arbitrary input types for the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajq3MZ0bhlxG",
    "outputId": "05c1c2ab-bf36-49b1-9cda-c9f1ff1f8abc"
   },
   "outputs": [],
   "source": [
    "runner.CACHE.RESET()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0il8pIX8H_uz"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def helper(model, X, y, history, device, loss_fn, optimizer=None):\n",
    "  for st in range(0, X.shape[0], 500):\n",
    "    en = min(st + 500, X.shape[0])\n",
    "    X_ = torch.from_numpy(X[st:en, :]).float().to(device)\n",
    "    pred = model(X_)\n",
    "    y_ = torch.from_numpy(y[st:en]).float().to(device)\n",
    "    loss = loss_fn(pred.squeeze(), y_)\n",
    "    if optimizer is not None:\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    history.append(loss.detach().cpu().item())\n",
    "    \n",
    "def my_custom_run(runner: ExperimentRunner, show_freq=5):\n",
    "  # print(runner.CACHE._cached_primitives)\n",
    "  # return\n",
    "  # Get model from cache\n",
    "  my_model = runner.CACHE.GET_M('mlp-key')\n",
    "\n",
    "  # Get the epoch number and history from cache\n",
    "  # if it is not cached from before set it to zero\n",
    "  epoch_i = runner.CACHE.SET_IFN('epoch_i', 0)\n",
    "  loss_history = runner.CACHE.SET_IFN('loss-history', [[], []])\n",
    "  writer = SummaryWriter(log_dir=runner.CACHE.LOGS_DIR)\n",
    "  inds = np.arange(runner.train_X.shape[0])\n",
    "  for epoch_i in range(epoch_i, 100):\n",
    "\n",
    "    np.random.shuffle(inds)\n",
    "\n",
    "    loss_train = []\n",
    "    my_model.train()\n",
    "    helper(my_model, runner.train_X[inds],\n",
    "           runner.train_Y[inds], loss_train, runner.device,\n",
    "           runner.criterion, runner.optim)\n",
    "    mean_loss = sum(loss_train) / len(loss_train)\n",
    "    loss_history[0].append(mean_loss)\n",
    "    train_loss = mean_loss\n",
    "    loss_test = []\n",
    "    my_model.eval()\n",
    "    with torch.no_grad():\n",
    "      helper(my_model, runner.test_X,\n",
    "            runner.test_Y, loss_test, runner.device,\n",
    "            runner.criterion)\n",
    "    mean_loss = sum(loss_test) / len(loss_test)\n",
    "    loss_history[1].append(mean_loss)\n",
    "    test_loss = mean_loss\n",
    "    writer.add_scalars('losses', {\n",
    "      'train': train_loss,\n",
    "      'test': test_loss\n",
    "    }, epoch_i)\n",
    "    if (epoch_i + 1) % show_freq == 0:\n",
    "      if runner.verbose > 0:\n",
    "        clear_output()\n",
    "        plt.plot(list(range(len(loss_history[0]))), loss_history[0], label='loss-train')\n",
    "        plt.plot(list(range(len(loss_history[1]))), loss_history[1], label='loss-test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Caching and saving checkpoints\n",
    "    runner.CACHE.SET('epoch_i', epoch_i)\n",
    "    runner.CACHE.SET('loss-history', loss_history)\n",
    "    runner.CACHE.SET_M('mlp-key', my_model)\n",
    "    runner.CACHE.SAVE()\n",
    "\n",
    "  return loss_history[0][-1]\n",
    "\n",
    "runner.implement_run(my_custom_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc8aGQq_jDAG"
   },
   "source": [
    "Run the following gridsearch algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ixAXNzv72XdZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no. [3/12] -- Running 1-0-1-MLM-conf-test-branches.yaml : 27561897743.058823\n",
      "Iteration no. [4/12] -- Running 2-1-0-MLM-conf-test-branches.yaml : 55214974494.117645\n",
      "Iteration no. [5/12] -- Running 0-0-1-MLM-conf-test-branches.yaml : 16666299060.705883\n",
      "Iteration no. [6/12] -- Running 0-0-0-MLM-conf-test-branches.yaml : 21333825897.411766\n",
      "Iteration no. [7/12] -- Running 0-1-0-MLM-conf-test-branches.yaml : 18080709089.882355\n",
      "Iteration no. [8/12] -- Running 1-1-1-MLM-conf-test-branches.yaml : 26806452886.588234\n",
      "Iteration no. [9/12] -- Running 1-1-0-MLM-conf-test-branches.yaml : 29983394876.235294\n",
      "Iteration no. [10/12] -- Running 2-0-1-MLM-conf-test-branches.yaml : 55504138601.411766\n",
      "Iteration no. [11/12] -- Running 2-1-1-MLM-conf-test-branches.yaml : 53756621884.23529\n"
     ]
    }
   ],
   "source": [
    "from mlmnemonist.validation_tools import grid_search\n",
    "grid_search(runner,\n",
    "            cache_token='mine',\n",
    "            verbose=1,\n",
    "            cfg_base=get_cfg_defaults(),\n",
    "            all_cfg_dir='config/all-branches')\n",
    "\n",
    "%autoreload 3\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ExperimentRunnerTutorial.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}