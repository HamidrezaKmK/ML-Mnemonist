{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HamidrezaKmK/ML-Mnemonist/blob/main/ExperimentRunnerTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hyper-Runners\n",
    "\n",
    "Hyper-runners are entities that act on top of normal runners and control them. While runners produce experiments, hyper-runners produce hyper-experiments. An example of hyper-experiment would be *hyper parameter tuning*. If you have a training algorithm with a tunable hyper parameter $\\lambda$, you might assign it multiple values $\\lambda_1, \\lambda_2, ..., \\lambda_k$ and run the experiment. The experiment providing the best results on validation would be the output of our hyper parameter tuning. In this example, the training example itself is the experiment and its result is the accuracy on the validation set. The hyper parameter tuning experiment is the hyper experiment which searches over experiment entries.\n",
    "\n",
    "In this tutorial, we will use the same runner from [Tutorial 1](./Tutorial-1.ipynb) and try to tune some of the properties of our MLP solver. While normal runners use configuration yaml files, hyper-experiments also use a configuration yaml file; however, these are referred to as configuration palettes. Configuration palettes have additional configuration nodes called `MLM_BRANCH`. These branches act as a palette to create multiple normal trees. To illustrate via an example, check out the following configuration palette:\n",
    "\n",
    "```\n",
    "DATASET:\n",
    "  TRAIN_NAME: 'california_housing_train.csv'\n",
    "  TEST_NAME: 'california_housing_test.csv'\n",
    "\n",
    "SOLVER:\n",
    "  LR:\n",
    "    MLM_BRANCH_1: 0.001\n",
    "    MLM_BRANCH_2: 0.0001\n",
    "  OPTIMIZER_TYPE: 'adam'\n",
    "  DEVICE: 'cpu'\n",
    "  METHOD: 'mlp-method'\n",
    "\n",
    "MODEL:\n",
    "  HYPER_PARAMETERS:\n",
    "    IN_FEATURES: 8\n",
    "    H1:\n",
    "      MLM_BRANCH_1: 100\n",
    "      MLM_BRANCH_2: 200\n",
    "    H2:\n",
    "      10\n",
    "```\n",
    "As you can see, some nodes are defined with `MLM_BRANCH_i` which means for example `LR` can be set to either `MLM_BRANCH_1` which is $0.001$ or `MLM_BRANCH_2` which is $0.0001$. At the same time, `H1` can either be set to `MLM_BRANCH_1` which is 100 or `MLM_BRANCH_2` which is 200. All in all, this will produce four different normal configurations with `LR = 0.001, H1 = 100`, `LR = 0.001, H1 = 200`, `LR = 0.0001, H1 = 100`, or `LR = 0.0001, H1 = 200`.\n",
    "\n",
    "Another more complex example of using branches is given below:\n",
    "```\n",
    "A:\n",
    "  MLM_BRANCH_1:\n",
    "    B:\n",
    "      MLM_BRANCH_1: 10\n",
    "      MLM_BRANCH_2: 100\n",
    "  MLM_BRANCH_2:\n",
    "    C:\n",
    "      MLM_BRANCH_1:\n",
    "        D: 5\n",
    "        E: 10\n",
    "      MLM_BRANCH_2:\n",
    "        F: 10\n",
    "```\n",
    "The end result will be the following 4 configurations:\n",
    "```\n",
    "A:\n",
    "  B: 10\n",
    "```\n",
    "```\n",
    "A:\n",
    "  B: 100\n",
    "```\n",
    "```\n",
    "A:\n",
    "  C:\n",
    "    D: 5\n",
    "    E: 10\n",
    "```\n",
    "```\n",
    "A:\n",
    "  C:\n",
    "    F: 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNiIuYQbWti3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Expanding configuration palettes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FxBXOOvtLWC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We note that the configuration palette is available at `conf-test-branches.yaml`. We can use the `expand_cfg` method to create all the actual configurations in a new directory. This function takes in a default configuration to start with and takes in a configuration palette directory and a save directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import os\n",
    "from mlmnemonist.validation_tools import expand_cfg\n",
    "from testing.config.config import get_cfg_defaults\n",
    "expand_cfg(get_cfg_defaults(),\n",
    "           cfg_dir='conf-test-branches.yaml',\n",
    "           save_directory=os.path.join('config', 'all-branches'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above cell, you can check out the yaml files produced in the corresponding directory; these yaml files represent all the possible configuration given that palette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a hyper-runner\n",
    "\n",
    "Now we will only load the runner from the previous tutorial but we need not use the configurations it used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "from mlp_functions import *\n",
    "import mlmnemonist as mlm\n",
    "from mlmnemonist import FACTORY\n",
    "from testing.config.config import get_cfg_defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "  runner = FACTORY.retrieve_experiment_runner('tut2')\n",
    "except FileNotFoundError as e:\n",
    "  runner = FACTORY.create_experiment_runner(\n",
    "    cache_token='tut2',\n",
    "    description='A sample lightweight runner',\n",
    "    cfg_base=get_cfg_defaults(),\n",
    "    cfg_dir='conf-test.yaml',\n",
    "    experiment_name='fullsearch-runner',\n",
    "    verbose=2,\n",
    "  )\n",
    "  runner.preprocessing_pipeline.update_function(load_raw_data)\n",
    "  runner.preprocessing_pipeline.update_function(process_data)\n",
    "  runner.recurring_pipeline.update_function(setup_device)\n",
    "  runner.recurring_pipeline.update_function(setup_model)\n",
    "  runner.recurring_pipeline.update_function(setup_training)\n",
    "  runner.implement_run(my_custom_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runner has implemented functions in [mlp_functions](./mlp_functions.py). Now we need to plug this runner to a hyper-runner. The hyper runner can handle a set of runners and will look at each of the runners as a blackbox and uses their output to conduct search. Following is a syntax of creating a new hyper-runner which is also done via the factory object provided.\n",
    "\n",
    "Note that hyper-runners are also cached and saved in the caching directory. This means that whenever a session crashes, the search does not need to start from the beginning and can continue from where it left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hyper_runner = FACTORY.create_hyper_experiment_runner(\n",
    "  experiment_name='cross-run',\n",
    "  cfg_base=get_cfg_defaults(),\n",
    "  cfg_palette_path='conf-test-branches.yaml',\n",
    "  experiment_runners=[runner],\n",
    "  verbose=4,\n",
    "  cache_token='tut2'\n",
    ")\n",
    "# Run the preprocessing pipeline\n",
    "hyper_runner.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixAXNzv72XdZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run the full search for a while. Try interrupting and re-running this cell\n",
    "hyper_runner.full_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Hyper-experiment\n",
    "\n",
    "Now we want to check whether after crashing the session, the framework is able to revive the hyper experiments or not. To do so, restart the whole kernel and run the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "%autoreload 2\n",
    "import os\n",
    "from testing.config.config import get_cfg_defaults\n",
    "from mlp_functions import *\n",
    "import mlmnemonist as mlm\n",
    "from mlmnemonist import FACTORY\n",
    "from testing.config.config import get_cfg_defaults\n",
    "\n",
    "hyper_runner = FACTORY.retrieve_hyper_experiment_runner('tut2')\n",
    "hyper_runner.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you run the following it will continue from where it left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_runner.full_search()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ExperimentRunnerTutorial.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ef0468b6e6b9a4e1c076af9f1e41a9e6221f7ddc2a937addc4d954af369e1a55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
